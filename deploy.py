"""
YOLOv11 ç”µåŠ›å®‰å…¨æ£€æµ‹éƒ¨ç½²è„šæœ¬
åŠŸèƒ½ï¼š
1. æ¨¡å‹æ¨ç†å’Œæ£€æµ‹
2. Webç•Œé¢éƒ¨ç½²
3. æ‰¹é‡å›¾ç‰‡æ£€æµ‹
4. è§†é¢‘æµæ£€æµ‹
5. å®æ—¶æ‘„åƒå¤´æ£€æµ‹

ä½¿ç”¨æ–¹æ³•ï¼š
- æ‰¹é‡æ£€æµ‹: python deploy.py --mode batch --source images/ --weights runs/detect/train/weights/best.pt
- Webç•Œé¢: python deploy.py --mode web --weights runs/detect/train/weights/best.pt
- è§†é¢‘æ£€æµ‹: python deploy.py --mode video --source video.mp4 --weights runs/detect/train/weights/best.pt
"""

import argparse
import cv2
import numpy as np
import torch
from ultralytics import YOLO
import streamlit as st
from PIL import Image
import os
import glob
from pathlib import Path
import time
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import json


class SafetyDetectionDeploy:
    def __init__(self, weights_path, conf_threshold=0.25, iou_threshold=0.45):
        self.weights_path = weights_path
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        self.model = None
        self.class_names = {
            0: 'person',
            1: 'helmet_person',
            2: 'insulated_gloves',
            3: 'safety_belt',
            4: 'power_pole',
            5: 'voltage_tester',
            6: 'work_clothes'
        }
        self.class_colors = {
            0: (0, 255, 0),      # person - ç»¿è‰²
            1: (0, 255, 255),    # helmet_person - é»„è‰²
            2: (255, 0, 255),    # insulated_gloves - å“çº¢
            3: (255, 255, 0),    # safety_belt - é’è‰²
            4: (128, 0, 128),    # power_pole - ç´«è‰²
            5: (255, 165, 0),    # voltage_tester - æ©™è‰²
            6: (128, 128, 0)     # work_clothes - æ£•è‰²
        }
        
        self.load_model()
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        print(f"åŠ è½½æ¨¡å‹: {self.weights_path}")
        try:
            self.model = YOLO(self.weights_path)
            print("æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def detect_single_image(self, image_path, save_path=None):
        """æ£€æµ‹å•å¼ å›¾ç‰‡"""
        # è¯»å–å›¾åƒ
        image = cv2.imread(image_path)
        if image is None:
            print(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")
            return None
        
        # è¿›è¡Œæ¨ç†
        results = self.model(image, conf=self.conf_threshold, iou=self.iou_threshold)
        
        # ç»˜åˆ¶æ£€æµ‹ç»“æœ
        annotated_image = results[0].plot()
        
        # ä¿å­˜ç»“æœ
        if save_path:
            cv2.imwrite(save_path, annotated_image)
        
        # æå–æ£€æµ‹ä¿¡æ¯
        detections = []
        for result in results:
            boxes = result.boxes
            for box in boxes:
                class_id = int(box.cls[0])
                confidence = float(box.conf[0])
                bbox = box.xyxy[0].tolist()
                
                detections.append({
                    'class_id': class_id,
                    'class_name': self.class_names.get(class_id, f'unknown_{class_id}'),
                    'confidence': confidence,
                    'bbox': bbox
                })
        
        return annotated_image, detections
    
    def batch_detect(self, source_dir, output_dir, image_extensions=['*.jpg', '*.jpeg', '*.png']):
        """æ‰¹é‡æ£€æµ‹å›¾ç‰‡"""
        print(f"å¼€å§‹æ‰¹é‡æ£€æµ‹: {source_dir} -> {output_dir}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
        image_files = []
        for ext in image_extensions:
            image_files.extend(glob.glob(os.path.join(source_dir, ext)))
            image_files.extend(glob.glob(os.path.join(source_dir, '**', ext), recursive=True))
        
        print(f"æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡")
        
        # ç»Ÿè®¡ä¿¡æ¯
        detection_stats = {class_id: 0 for class_id in self.class_names.keys()}
        results_log = []
        
        # å¤„ç†æ¯å¼ å›¾ç‰‡
        for i, image_path in enumerate(image_files):
            print(f"å¤„ç†å›¾ç‰‡ {i+1}/{len(image_files)}: {os.path.basename(image_path)}")
            
            try:
                # æ£€æµ‹
                annotated_image, detections = self.detect_single_image(image_path)
                
                # æ›´æ–°ç»Ÿè®¡
                for detection in detections:
                    detection_stats[detection['class_id']] += 1
                
                # ä¿å­˜ç»“æœ
                output_path = os.path.join(output_dir, f"detect_{os.path.basename(image_path)}")
                cv2.imwrite(output_path, annotated_image)
                
                # è®°å½•æ—¥å¿—
                results_log.append({
                    'image_name': os.path.basename(image_path),
                    'detections': detections,
                    'timestamp': datetime.now().isoformat()
                })
                
            except Exception as e:
                print(f"å¤„ç† {image_path} æ—¶å‡ºé”™: {e}")
        
        # ä¿å­˜ç»Ÿè®¡ç»“æœ
        self.save_detection_stats(detection_stats, results_log, output_dir)
        
        print("æ‰¹é‡æ£€æµ‹å®Œæˆï¼")
        return detection_stats, results_log
    
    def video_detect(self, video_source, output_path=None, show_fps=True):
        """è§†é¢‘æ£€æµ‹"""
        print(f"å¼€å§‹è§†é¢‘æ£€æµ‹: {video_source}")
        
        # æ‰“å¼€è§†é¢‘
        if video_source.isdigit():
            video_source = int(video_source)
        
        cap = cv2.VideoCapture(video_source)
        if not cap.isOpened():
            print(f"æ— æ³•æ‰“å¼€è§†é¢‘æº: {video_source}")
            return
        
        # è·å–è§†é¢‘ä¿¡æ¯
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        print(f"è§†é¢‘ä¿¡æ¯: {width}x{height}, {fps}fps")
        
        # è®¾ç½®è¾“å‡ºè§†é¢‘
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'XVID')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        # æ£€æµ‹ç»Ÿè®¡
        detection_stats = {class_id: 0 for class_id in self.class_names.keys()}
        frame_count = 0
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                
                # æ£€æµ‹
                start_time = time.time()
                results = self.model(frame, conf=self.conf_threshold, iou=self.iou_threshold)
                detection_time = time.time() - start_time
                
                # ç»˜åˆ¶ç»“æœ
                annotated_frame = results[0].plot()
                
                # æ›´æ–°ç»Ÿè®¡
                for result in results:
                    for box in result.boxes:
                        class_id = int(box.cls[0])
                        detection_stats[class_id] += 1
                
                # æ˜¾ç¤ºFPS
                if show_fps:
                    fps_text = f"FPS: {1/detection_time:.1f}"
                    cv2.putText(annotated_frame, fps_text, (10, 30), 
                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                
                # æ˜¾ç¤ºå¸§
                cv2.imshow('Safety Detection', annotated_frame)
                
                # ä¿å­˜åˆ°è¾“å‡ºè§†é¢‘
                if output_path:
                    out.write(annotated_frame)
                
                # æŒ‰'q'é€€å‡º
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
        
        except KeyboardInterrupt:
            print("ç”¨æˆ·ä¸­æ–­æ£€æµ‹")
        
        finally:
            cap.release()
            if output_path:
                out.release()
            cv2.destroyAllWindows()
        
        print(f"è§†é¢‘æ£€æµ‹å®Œæˆï¼Œå…±å¤„ç† {frame_count} å¸§")
        print("æ£€æµ‹ç»Ÿè®¡:")
        for class_id, count in detection_stats.items():
            class_name = self.class_names.get(class_id, f'unknown_{class_id}')
            print(f"  {class_name}: {count} æ¬¡æ£€æµ‹")
        
        return detection_stats
    
    def save_detection_stats(self, stats, results_log, output_dir):
        """ä¿å­˜æ£€æµ‹ç»Ÿè®¡ç»“æœ"""
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_file = os.path.join(output_dir, 'detection_stats.json')
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜è¯¦ç»†æ—¥å¿—
        log_file = os.path.join(output_dir, 'detection_log.json')
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(results_log, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆç»Ÿè®¡å›¾è¡¨
        self.generate_stats_charts(stats, output_dir)
        
        print(f"ç»Ÿè®¡ç»“æœå·²ä¿å­˜è‡³: {output_dir}")
    
    def generate_stats_charts(self, stats, output_dir):
        """ç”Ÿæˆç»Ÿè®¡å›¾è¡¨"""
        # å‡†å¤‡æ•°æ®
        class_names_list = [self.class_names.get(k, f'unknown_{k}') for k in stats.keys()]
        counts = list(stats.values())
        
        # åˆ›å»ºæŸ±çŠ¶å›¾
        fig = px.bar(
            x=class_names_list,
            y=counts,
            labels={'x': 'ç±»åˆ«', 'y': 'æ£€æµ‹æ¬¡æ•°'},
            title='æ£€æµ‹ç»“æœç»Ÿè®¡',
            color=counts,
            color_continuous_scale='viridis'
        )
        
        chart_path = os.path.join(output_dir, 'detection_stats.html')
        fig.write_html(chart_path)
        
        print(f"ç»Ÿè®¡å›¾è¡¨å·²ä¿å­˜è‡³: {chart_path}")


def web_interface():
    """Streamlit Webç•Œé¢"""
    st.set_page_config(
        page_title="YOLOv11 ç”µåŠ›å®‰å…¨æ£€æµ‹",
        page_icon="âš¡",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("âš¡ YOLOv11 ç”µåŠ›å®‰å…¨æ£€æµ‹ç³»ç»Ÿ")
    st.markdown("åŸºäºYOLOv11çš„ç”µåŠ›ä½œä¸šå®‰å…¨è£…å¤‡æ£€æµ‹")
    
    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("æ£€æµ‹é…ç½®")
        weights_path = st.text_input("æ¨¡å‹è·¯å¾„", value="runs/detect/train/weights/best.pt")
        conf_threshold = st.slider("ç½®ä¿¡åº¦é˜ˆå€¼", 0.0, 1.0, 0.25)
        iou_threshold = st.slider("IoUé˜ˆå€¼", 0.0, 1.0, 0.45)
        
        # åŠ è½½æ¨¡å‹
        if st.button("åŠ è½½æ¨¡å‹"):
            try:
                detector = SafetyDetectionDeploy(weights_path, conf_threshold, iou_threshold)
                st.session_state.detector = detector
                st.success("æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            except Exception as e:
                st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        st.divider()
        st.header("åŠŸèƒ½é€‰æ‹©")
        mode = st.selectbox("é€‰æ‹©æ¨¡å¼", ["å›¾ç‰‡æ£€æµ‹", "æ‰¹é‡æ£€æµ‹", "è§†é¢‘æ£€æµ‹"])
    
    # ä¸»ç•Œé¢
    if mode == "å›¾ç‰‡æ£€æµ‹":
        image_detection_tab()
    elif mode == "æ‰¹é‡æ£€æµ‹":
        batch_detection_tab()
    elif mode == "è§†é¢‘æ£€æµ‹":
        video_detection_tab()


def image_detection_tab():
    """å›¾ç‰‡æ£€æµ‹æ ‡ç­¾é¡µ"""
    st.header("ğŸ–¼ï¸ å•å¼ å›¾ç‰‡æ£€æµ‹")
    
    if 'detector' not in st.session_state:
        st.warning("è¯·å…ˆåœ¨ä¾§è¾¹æ åŠ è½½æ¨¡å‹")
        return
    
    detector = st.session_state.detector
    
    # ä¸Šä¼ å›¾ç‰‡
    uploaded_file = st.file_uploader("é€‰æ‹©å›¾ç‰‡", type=['jpg', 'jpeg', 'png'])
    
    if uploaded_file is not None:
        # æ˜¾ç¤ºåŸå§‹å›¾ç‰‡
        image = Image.open(uploaded_file)
        st.image(image, caption="åŸå§‹å›¾ç‰‡", use_column_width=True)
        
        # æ£€æµ‹æŒ‰é’®
        if st.button("å¼€å§‹æ£€æµ‹"):
            with st.spinner("æ­£åœ¨æ£€æµ‹..."):
                # è½¬æ¢ä¸ºOpenCVæ ¼å¼
                image_np = np.array(image)
                image_cv = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)
                
                # æ£€æµ‹
                results = detector.model(image_cv, conf=detector.conf_threshold, iou=detector.iou_threshold)
                
                # ç»˜åˆ¶ç»“æœ
                annotated_image = results[0].plot()
                annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)
                
                # æ˜¾ç¤ºç»“æœ
                st.image(annotated_image_rgb, caption="æ£€æµ‹ç»“æœ", use_column_width=True)
                
                # æ˜¾ç¤ºæ£€æµ‹è¯¦æƒ…
                with st.expander("æ£€æµ‹è¯¦æƒ…"):
                    for i, result in enumerate(results):
                        boxes = result.boxes
                        st.write(f"**æ£€æµ‹åˆ° {len(boxes)} ä¸ªç›®æ ‡:**")
                        
                        for j, box in enumerate(boxes):
                            class_id = int(box.cls[0])
                            confidence = float(box.conf[0])
                            class_name = detector.class_names.get(class_id, f'unknown_{class_id}')
                            
                            st.write(f"- ç›®æ ‡ {j+1}: {class_name} (ç½®ä¿¡åº¦: {confidence:.2%})")


def batch_detection_tab():
    """æ‰¹é‡æ£€æµ‹æ ‡ç­¾é¡µ"""
    st.header("ğŸ“ æ‰¹é‡å›¾ç‰‡æ£€æµ‹")
    
    if 'detector' not in st.session_state:
        st.warning("è¯·å…ˆåœ¨ä¾§è¾¹æ åŠ è½½æ¨¡å‹")
        return
    
    detector = st.session_state.detector
    
    # è¾“å…¥è¾“å‡ºè·¯å¾„
    source_dir = st.text_input("è¾“å…¥å›¾ç‰‡ç›®å½•")
    output_dir = st.text_input("è¾“å‡ºç›®å½•", value="detection_results")
    
    if st.button("å¼€å§‹æ‰¹é‡æ£€æµ‹"):
        if not source_dir or not os.path.exists(source_dir):
            st.error("è¯·è¾“å…¥æœ‰æ•ˆçš„è¾“å…¥ç›®å½•")
        else:
            with st.spinner("æ­£åœ¨æ‰¹é‡æ£€æµ‹..."):
                stats, results_log = detector.batch_detect(source_dir, output_dir)
            
            st.success(f"æ‰¹é‡æ£€æµ‹å®Œæˆï¼å…±å¤„ç† {len(results_log)} å¼ å›¾ç‰‡")
            
            # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
            st.subheader("æ£€æµ‹ç»Ÿè®¡")
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**å„ç±»åˆ«æ£€æµ‹æ•°é‡:**")
                for class_id, count in stats.items():
                    class_name = detector.class_names.get(class_id, f'unknown_{class_id}')
                    st.write(f"- {class_name}: {count}")
            
            with col2:
                # ç»˜åˆ¶é¥¼å›¾
                fig = px.pie(
                    values=list(stats.values()),
                    names=[detector.class_names.get(k, f'unknown_{k}') for k in stats.keys()],
                    title="æ£€æµ‹åˆ†å¸ƒ"
                )
                st.plotly_chart(fig, use_container_width=True)


def video_detection_tab():
    """è§†é¢‘æ£€æµ‹æ ‡ç­¾é¡µ"""
    st.header("ğŸ“¹ è§†é¢‘æ£€æµ‹")
    
    if 'detector' not in st.session_state:
        st.warning("è¯·å…ˆåœ¨ä¾§è¾¹æ åŠ è½½æ¨¡å‹")
        return
    
    detector = st.session_state.detector
    
    st.info("è§†é¢‘æ£€æµ‹éœ€è¦åœ¨æœ¬åœ°è¿è¡Œï¼Œè¯·ä½¿ç”¨å‘½ä»¤è¡Œæ¨¡å¼")
    st.code("python deploy.py --mode video --source video.mp4 --weights best.pt")
    
    # å‚æ•°è¯´æ˜
    st.subheader("å‚æ•°è¯´æ˜")
    st.write("- `--source`: è§†é¢‘æ–‡ä»¶è·¯å¾„æˆ–æ‘„åƒå¤´ID")
    st.write("- `--weights`: æ¨¡å‹æƒé‡è·¯å¾„")
    st.write("- `--output`: è¾“å‡ºè§†é¢‘è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    st.write("- `--conf`: ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.25ï¼‰")


def main():
    parser = argparse.ArgumentParser(description='YOLOv11ç”µåŠ›å®‰å…¨æ£€æµ‹éƒ¨ç½²')
    parser.add_argument('--mode', type=str, default='web', 
                       choices=['web', 'batch', 'video', 'image'],
                       help='éƒ¨ç½²æ¨¡å¼')
    parser.add_argument('--weights', type=str, default='runs/detect/train/weights/best.pt',
                       help='æ¨¡å‹æƒé‡è·¯å¾„')
    parser.add_argument('--source', type=str, help='è¾“å…¥æºï¼ˆå›¾ç‰‡/è§†é¢‘è·¯å¾„æˆ–ç›®å½•ï¼‰')
    parser.add_argument('--output', type=str, default='detection_results',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--conf', type=float, default=0.25,
                       help='ç½®ä¿¡åº¦é˜ˆå€¼')
    parser.add_argument('--iou', type=float, default=0.45,
                       help='IoUé˜ˆå€¼')
    
    args = parser.parse_args()
    
    if args.mode == 'web':
        web_interface()
    elif args.mode == 'batch':
        if not args.source:
            print("è¯·æä¾›è¾“å…¥ç›®å½•: --source")
            return
        detector = SafetyDetectionDeploy(args.weights, args.conf, args.iou)
        detector.batch_detect(args.source, args.output)
    elif args.mode == 'video':
        if not args.source:
            print("è¯·æä¾›è§†é¢‘æº: --source")
            return
        detector = SafetyDetectionDeploy(args.weights, args.conf, args.iou)
        detector.video_detect(args.source, args.output)
    elif args.mode == 'image':
        if not args.source:
            print("è¯·æä¾›å›¾ç‰‡è·¯å¾„: --source")
            return
        detector = SafetyDetectionDeploy(args.weights, args.conf, args.iou)
        annotated_image, detections = detector.detect_single_image(args.source, args.output)
        
        print("æ£€æµ‹ç»“æœ:")
        for detection in detections:
            print(f"  {detection['class_name']}: {detection['confidence']:.2%}")


if __name__ == '__main__':
    main()
